"""
Streamlit –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è ALPACA Test Bench.
"""

import importlib.util
import json
import platform
import time
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="ALPACA Test Bench",
    page_icon="üß™",
    layout="wide",
    initial_sidebar_state="expanded"
)

# –ò–º–ø–æ—Ä—Ç—ã –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
try:
    import sys
    from pathlib import Path

    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path
    project_root = Path(__file__).parent.parent.parent
    sys.path.insert(0, str(project_root))
    
    from configs.processors_config import ALL_PROCESSORS, QUALITY_METRICS
    from src.core.pipeline import DocumentPipeline
    from src.processors.docx_processors import (Docx2txtExtractor,
                                                DocExtractor,
                                                PythonDocxExtractor,
                                                Win32WordExtractor)
    from src.processors.markdown_converters import (CustomMarkdownFormatter,
                                                    Html2TextConverter,
                                                    MarkdownifyConverter,
                                                    PandocConverter)
    from src.processors.pdf_processors import (PDFPlumberExtractor,
                                               PyMuPDFExtractor,
                                               PyPDFExtractor)
    from src.processors.text_cleaners import (AdvancedTextCleaner,
                                              BasicTextCleaner, HTMLCleaner)
    from src.utils.file_manager import FileManager
    from src.utils.llm_client import LLMConfig, build_llm_callable
    from src.utils.logger import get_logger, setup_logging

    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã
    try:
        from src.processors.unstructured_processors import (
            UnstructuredLLMCleaner, UnstructuredPartitionExtractor)
        UNSTRUCTURED_AVAILABLE = True
    except ImportError:
        UNSTRUCTURED_AVAILABLE = False
    
    setup_logging("INFO")
    logger = get_logger(__name__)
    
except ImportError as e:
    st.error(f"–û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
    st.stop()


class StreamlitApp:
    """–û—Å–Ω–æ–≤–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ Streamlit."""
    
    def __init__(self):
        self.pipeline = DocumentPipeline()
        self.file_manager = FileManager()
        self.unstructured_available = UNSTRUCTURED_AVAILABLE
        self.setup_pipeline()
        self._ensure_llm_state()

    @staticmethod
    def _default_llm_state() -> Dict[str, Any]:
        return {
            "enabled": False,
            "provider": "openai",
            "model": "gpt-4o-mini",
            "api_key": "",
            "base_url": "",
            "temperature": 0.0,
            "system_prompt": "",
            "timeout": 60,
            "max_output_tokens": 512,
            "chunk_size": 2048,
        }

    def _ensure_llm_state(self) -> Dict[str, Any]:
        if "llm_settings" not in st.session_state:
            st.session_state["llm_settings"] = self._default_llm_state()
        return st.session_state["llm_settings"]
        
    def setup_pipeline(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç pipeline —Å –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞–º–∏."""
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º PDF –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã
        try:
            self.pipeline.register_extractor(['.pdf'], PyPDFExtractor())
            self.pipeline.register_extractor(['.pdf'], PDFPlumberExtractor())
            self.pipeline.register_extractor(['.pdf'], PyMuPDFExtractor())
        except Exception as e:
            st.warning(f"–ù–µ–∫–æ—Ç–æ—Ä—ã–µ PDF –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã: {e}")
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º Word –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã
        try:
            self.pipeline.register_extractor(['.docx'], PythonDocxExtractor())
            self.pipeline.register_extractor(['.docx'], Docx2txtExtractor())
        except Exception as e:
            st.warning(f"Docx –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã: {e}")

        # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –¥–ª—è .doc (–ø—Ä–æ–±—É–µ—Ç antiword, catdoc, LibreOffice, MS Word)
        try:
            self.pipeline.register_extractor(['.doc'], DocExtractor())
        except Exception as e:
            st.warning(f"DOC —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")

        try:
            win32_available = (
                platform.system().lower().startswith("win") and
                importlib.util.find_spec("win32com") is not None
            )
            if win32_available:
                self.pipeline.register_extractor(['.doc', '.docx'], Win32WordExtractor())
            else:
                logger.info("pywin32/MS Word COM –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ: –º–æ–¥—É–ª—å win32com –Ω–µ –Ω–∞–π–¥–µ–Ω")
        except Exception as e:
            st.warning(f"MS Word COM —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")

        # Unstructured –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
        if self.unstructured_available:
            try:
                unstructured_types = [
                    '.pdf',
                    '.docx',
                    '.doc',
                    '.pptx',
                    '.ppt',
                    '.md',
                    '.html',
                ]
                language_hints = ['rus', 'eng']
                unstructured_params = {
                    "supported_types": unstructured_types,
                    "strategy": "hi_res",
                    "chunking_strategy": "by_title",
                    "include_metadata": True,
                    "infer_table_structure": True,
                    "languages": language_hints,
                    "partition_kwargs": {
                        "languages": language_hints,
                        "ocr_languages": 'rus+eng',
                    },
                    "fallback_partition_kwargs": {
                        "languages": language_hints,
                        "ocr_languages": 'rus+eng',
                    },
                }
                self.pipeline.register_extractor(
                    unstructured_types,
                    UnstructuredPartitionExtractor(unstructured_params)
                )
                self.pipeline.register_cleaner(
                    UnstructuredLLMCleaner(
                        {
                            "use_llm_cleaning": False,
                            "repartition_if_missing": True,
                            "languages": language_hints,
                            "fallback_partition_kwargs": {
                                "languages": language_hints,
                                "ocr_languages": 'rus+eng',
                            },
                        }
                    )
                )
            except Exception as e:
                st.warning(f"Unstructured –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}")
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –æ—á–∏—Å—Ç–∏—Ç–µ–ª–∏
        self.pipeline.register_cleaner(BasicTextCleaner())
        self.pipeline.register_cleaner(AdvancedTextCleaner())
        self.pipeline.register_cleaner(HTMLCleaner())
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä—ã
        self.pipeline.register_converter("custom", CustomMarkdownFormatter())
        self.pipeline.register_converter("markdownify", MarkdownifyConverter())
        self.pipeline.register_converter("html2text", Html2TextConverter())
        self.pipeline.register_converter("pandoc", PandocConverter())

    def _get_unstructured_cleaner(self) -> Optional[Any]:
        for cleaner in self.pipeline.cleaners:
            if getattr(cleaner, 'name', '') == "Unstructured LLM Cleaner":
                return cleaner
        return None

    def _configure_unstructured_llm_cleaner(
        self,
        llm_required: bool,
        llm_callable: Optional[Callable[[str], str]],
    ) -> None:
        from src.utils.logger import logger
        cleaner = self._get_unstructured_cleaner()
        if cleaner is None:
            logger.warning("UnstructuredLLMCleaner –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ pipeline")
            return

        llm_state = self._ensure_llm_state()
        should_enable = bool(llm_required and llm_callable is not None)
        cleaner.config["use_llm_cleaning"] = should_enable
        cleaner.config["chunk_size"] = llm_state.get("chunk_size", cleaner.config.get("chunk_size", 2048))
        cleaner.llm_callable = llm_callable
        logger.info(f"–ù–∞—Å—Ç—Ä–æ–µ–Ω UnstructuredLLMCleaner: use_llm_cleaning={should_enable}, llm_callable={'—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω' if llm_callable else '–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'}")

    def _resolve_llm_callable(self) -> Optional[Callable[[str], str]]:
        from src.utils.logger import logger
        llm_state = self._ensure_llm_state()
        if not llm_state.get("enabled"):
            logger.info("LLM –≤—ã–∫–ª—é—á–µ–Ω –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö")
            return None

        api_key = (llm_state.get("api_key") or "").strip()
        model = (llm_state.get("model") or "").strip()
        if not api_key or not model:
            logger.error("–ù–µ —É–∫–∞–∑–∞–Ω API –∫–ª—é—á –∏–ª–∏ –º–æ–¥–µ–ª—å")
            raise ValueError("–£–∫–∞–∂–∏—Ç–µ API –∫–ª—é—á –∏ –º–æ–¥–µ–ª—å –¥–ª—è LLM.")

        provider = (llm_state.get("provider") or "openai").strip().lower()
        config = LLMConfig(
            provider=provider,
            model=model,
            api_key=api_key,
            base_url=(llm_state.get("base_url") or "").strip() or None,
            temperature=float(llm_state.get("temperature", 0.0)),
            system_prompt=llm_state.get("system_prompt") or None,
            timeout=int(llm_state.get("timeout", 60)),
            max_output_tokens=(llm_state.get("max_output_tokens") or None),
        )
        logger.info(f"–°–æ–∑–¥–∞–Ω LLM callable –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ {provider}, –º–æ–¥–µ–ª—å {model}")
        callable_fn = build_llm_callable(config)
        return callable_fn

    def _render_llm_settings(self) -> None:
        if not self.unstructured_available:
            return

        llm_state = self._ensure_llm_state()
        st.subheader("LLM –æ—á–∏—Å—Ç–∫–∞ (unstructured)")
        with st.form("llm_settings_form"):
            enabled = st.checkbox(
                "–í–∫–ª—é—á–∏—Ç—å LLM –æ—á–∏—Å—Ç–∫—É",
                value=llm_state.get("enabled", False),
            )
            provider = st.selectbox(
                "–ü—Ä–æ–≤–∞–π–¥–µ—Ä",
                ["OpenAI"],
                index=0,
            )
            model = st.text_input(
                "–ú–æ–¥–µ–ª—å",
                value=llm_state.get("model", "gpt-4o-mini"),
                help="–ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏, –¥–æ—Å—Ç—É–ø–Ω–æ–π –≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–µ",
            )
            api_key = st.text_input(
                "API –∫–ª—é—á",
                value=llm_state.get("api_key", ""),
                type="password",
                help="–ö–ª—é—á –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –Ω–∞ –¥–∏—Å–∫ –∏ –∂–∏–≤–µ—Ç —Ç–æ–ª—å–∫–æ –≤ —Å–µ—Å—Å–∏–∏ Streamlit",
            )
            base_url = st.text_input(
                "–ë–∞–∑–æ–≤—ã–π URL",
                value=llm_state.get("base_url", ""),
                placeholder="https://api.openai.com/v1/chat/completions",
            )
            temperature = st.slider(
                "Temperature",
                min_value=0.0,
                max_value=1.0,
                value=float(llm_state.get("temperature", 0.0)),
                step=0.05,
            )
            max_tokens = st.number_input(
                "Max output tokens (0 = –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)",
                min_value=0,
                max_value=8192,
                value=int(llm_state.get("max_output_tokens", 512) or 0),
            )
            chunk_size = st.number_input(
                "–†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ",
                min_value=256,
                max_value=4096,
                step=256,
                value=int(llm_state.get("chunk_size", 2048)),
            )
            timeout = st.number_input(
                "–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ (—Å–µ–∫)",
                min_value=10,
                max_value=180,
                value=int(llm_state.get("timeout", 60)),
            )
            system_prompt = st.text_area(
                "–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç",
                value=llm_state.get("system_prompt", ""),
                height=150,
            )

            submitted = st.form_submit_button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å LLM –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")

        if submitted:
            st.session_state["llm_settings"] = {
                "enabled": enabled,
                "provider": provider.lower(),
                "model": model.strip(),
                "api_key": api_key.strip(),
                "base_url": base_url.strip(),
                "temperature": float(temperature),
                "system_prompt": system_prompt.strip(),
                "timeout": int(timeout),
                "max_output_tokens": int(max_tokens) if max_tokens else None,
                "chunk_size": int(chunk_size),
            }
            st.success("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ LLM —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")

    def run(self):
        """–ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è."""
        if not self.unstructured_available:
            st.sidebar.info(
                "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–∞–∫–µ—Ç `unstructured[all-docs]`, —á—Ç–æ–±—ã –≤–∫–ª—é—á–∏—Ç—å"
                " —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ LLM-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é."
            )
        st.title("üß™ ALPACA Test Bench")
        st.subheader("–ò—Å–ø—ã—Ç–∞—Ç–µ–ª—å–Ω—ã–π —Å—Ç–µ–Ω–¥ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫ –æ—Ü–∏—Ñ—Ä–æ–≤–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        # Sidebar –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
        page = st.sidebar.selectbox(
            "–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã",
            [
                "üìÑ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞",
                "üîÑ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤", 
                "üìä –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞",
                "‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏",
                "üìà –ê–Ω–∞–ª–∏—Ç–∏–∫–∞"
            ]
        )
        
        if page == "üìÑ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞":
            self.single_file_testing()
        elif page == "üîÑ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤":
            self.processor_comparison()
        elif page == "üìä –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞":
            self.batch_processing()
        elif page == "‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏":
            self.settings_page()
        elif page == "üìà –ê–Ω–∞–ª–∏—Ç–∏–∫–∞":
            self.analytics_page()
    
    def single_file_testing(self):
        """–°—Ç—Ä–∞–Ω–∏—Ü–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞."""
        st.header("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞
        uploaded_file = st.file_uploader(
            "–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏",
            type=['pdf', 'docx', 'doc', 'pptx', 'ppt', 'xlsx', 'xls', 'jpg', 'jpeg', 'png'],
            help="–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: PDF, Word, PowerPoint, Excel, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"
        )
        
        if not uploaded_file:
            st.info("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
            return
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        temp_file = Path(f"temp_{uploaded_file.name}")
        temp_file.write_bytes(uploaded_file.getvalue())
        
        try:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                
                # –í—ã–±–æ—Ä –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤
                available_processors = self._get_available_processors(temp_file.suffix)
                selected_processors = st.multiselect(
                    "–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è",
                    available_processors,
                    default=available_processors[:2] if available_processors else []
                )
                
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ—á–∏—Å—Ç–∫–∏
                enable_cleaning = st.checkbox("–í–∫–ª—é—á–∏—Ç—å –æ—á–∏—Å—Ç–∫—É —Ç–µ–∫—Å—Ç–∞", value=True)
                if enable_cleaning:
                    cleaning_choices = [
                        "Basic Cleaner",
                        "Advanced Cleaner",
                        "HTML Cleaner",
                    ]
                    if self.unstructured_available:
                        cleaning_choices.append("Unstructured LLM Cleaner")
                    cleaning_options = st.multiselect(
                        "–ú–µ—Ç–æ–¥—ã –æ—á–∏—Å—Ç–∫–∏",
                        cleaning_choices,
                        default=["Basic Cleaner", "Advanced Cleaner"],
                    )
                else:
                    cleaning_options = []
                
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
                converter_options = st.multiselect(
                    "–ö–æ–Ω–≤–µ—Ä—Ç–µ—Ä—ã –≤ Markdown",
                    ["custom", "markdownify", "html2text", "pandoc"],
                    default=["custom"]
                )
            
            with col2:
                st.subheader("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ")
                
                file_info = {
                    "–ò–º—è —Ñ–∞–π–ª–∞": uploaded_file.name,
                    "–†–∞–∑–º–µ—Ä": f"{len(uploaded_file.getvalue()) / 1024:.1f} –ö–ë",
                    "–¢–∏–ø": uploaded_file.type or "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
                }
                
                for key, value in file_info.items():
                    st.text(f"{key}: {value}")
            
            # –ö–Ω–æ–ø–∫–∞ –∑–∞–ø—É—Å–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            if st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É", type="primary"):
                if not selected_processors:
                    st.error("–í—ã–±–µ—Ä–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä")
                    return
                
                self._process_single_file(
                    temp_file, 
                    selected_processors,
                    cleaning_options,
                    converter_options
                )
        
        finally:
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            if temp_file.exists():
                temp_file.unlink()
    
    def _get_available_processors(self, file_extension: str) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ –¥–ª—è —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞."""
        if not file_extension:
            return []
        processors = self.pipeline.get_extractors_for_type(file_extension.lower())
        return processors
    
    def _process_single_file(
        self, 
        file_path: Path, 
        processors: List[str],
        cleaners: List[str],
        converters: List[str]
    ):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–∞–π–ª –∏ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã."""
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        results = {}

        llm_required = (
            self.unstructured_available and "Unstructured LLM Cleaner" in cleaners
        )
        llm_callable: Optional[Callable[[str], str]] = None

        if llm_required:
            try:
                llm_callable = self._resolve_llm_callable()
            except Exception as exc:
                st.warning(f"LLM –æ—á–∏—Å—Ç–∫–∞ –Ω–µ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–∞: {exc}")
                llm_callable = None

        self._configure_unstructured_llm_cleaner(llm_required, llm_callable)

        if llm_required and llm_callable is None:
            st.info("Unstructured LLM Cleaner –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –±–µ–∑ LLM ‚Äî –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞.")
        
        for i, processor in enumerate(processors):
            status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å –ø–æ–º–æ—â—å—é {processor}...")
            progress_bar.progress((i + 1) / len(processors))
            
            try:
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞
                result = self.pipeline.process_document(
                    file_path=file_path,
                    extractor_name=processor,
                    cleaner_names=cleaners,
                    converter_name=converters[0] if converters else None,
                    llm_callable=llm_callable,
                )
                
                results[processor] = result
                
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å {processor}: {e}")
                logger.error(f"Processing failed with {processor}: {e}")
        
        status_text.text("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        progress_bar.progress(100)
        
        if results:
            self._display_results(results, file_path.name)
    
    def _display_results(self, results: Dict[str, Any], filename: str):
        """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
        st.header("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≤–∏–¥–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        tabs = st.tabs(["üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ", "üìù –¢–µ–∫—Å—Ç—ã", "üéØ –ö–∞—á–µ—Å—Ç–≤–æ", "üìà –ú–µ—Ç—Ä–∏–∫–∏"])
        
        with tabs[0]:
            self._display_comparison_results(results)
        
        with tabs[1]:
            self._display_extracted_texts(results)
        
        with tabs[2]:
            self._display_quality_scores(results)
        
        with tabs[3]:
            self._display_detailed_metrics(results)
    
    def _display_comparison_results(self, results: Dict[str, Any]):
        """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã."""
        st.subheader("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤")
        
        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        comparison_data = []
        
        for processor_name, result in results.items():
            quality_scores = result.get("quality_scores", {})
            
            # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
            best_score = 0.0
            best_combination = "N/A"
            execution_time = result.get("pipeline_metadata", {}).get("total_time", 0)
            
            for extractor in quality_scores:
                for cleaner in quality_scores[extractor]:
                    for converter in quality_scores[extractor][cleaner]:
                        score = quality_scores[extractor][cleaner][converter]
                        if hasattr(score, 'overall_score') and score.overall_score > best_score:
                            best_score = score.overall_score
                            best_combination = f"{extractor}‚Üí{cleaner}‚Üí{converter}"
            
            comparison_data.append({
                "–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä": processor_name,
                "–õ—É—á—à–∞—è –æ—Ü–µ–Ω–∫–∞": best_score,
                "–õ—É—á—à–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è": best_combination,
                "–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è": execution_time,
                "–û—Ü–µ–Ω–∫–∞": self._get_grade(best_score)
            })
        
        if comparison_data:
            df = pd.DataFrame(comparison_data)
            
            # –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            st.dataframe(df, use_container_width=True)
            
            # –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            col1, col2 = st.columns(2)
            
            with col1:
                # –ì—Ä–∞—Ñ–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞
                fig_quality = px.bar(
                    df, 
                    x="–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä", 
                    y="–õ—É—á—à–∞—è –æ—Ü–µ–Ω–∫–∞",
                    title="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏",
                    color="–õ—É—á—à–∞—è –æ—Ü–µ–Ω–∫–∞",
                    color_continuous_scale="RdYlGn"
                )
                st.plotly_chart(fig_quality, use_container_width=True)
            
            with col2:
                # –ì—Ä–∞—Ñ–∏–∫ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
                fig_time = px.bar(
                    df,
                    x="–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä",
                    y="–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è",
                    title="–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (—Å–µ–∫—É–Ω–¥—ã)",
                    color="–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è",
                    color_continuous_scale="RdYlBu_r"
                )
                st.plotly_chart(fig_time, use_container_width=True)
    
    def _display_extracted_texts(self, results: Dict[str, Any]):
        """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã."""
        st.subheader("–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã")
        
        for processor_name, result in results.items():
            with st.expander(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {processor_name}"):
                
                extraction_results = result.get("extraction_results", {})
                conversion_results = result.get("conversion_results", {})
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                st.subheader("–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç")
                for extractor_name, extraction_result in extraction_results.items():
                    if extraction_result.status.value == "completed":
                        text = extraction_result.content
                        st.text_area(
                            f"–ò–∑–≤–ª–µ—á–µ–Ω–æ —Å –ø–æ–º–æ—â—å—é {extractor_name}",
                            value=text[:1000] + "..." if len(text) > 1000 else text,
                            height=150,
                            key=f"extracted_{processor_name}_{extractor_name}"
                        )
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π Markdown
                st.subheader("–§–∏–Ω–∞–ª—å–Ω—ã–π Markdown")
                for extractor in conversion_results:
                    for cleaner in conversion_results[extractor]:
                        for converter in conversion_results[extractor][cleaner]:
                            conv_result = conversion_results[extractor][cleaner][converter]
                            if conv_result.status.value == "completed":
                                markdown = conv_result.content
                                st.code(markdown[:1500] + "..." if len(markdown) > 1500 else markdown, language="markdown")
    
    def _display_quality_scores(self, results: Dict[str, Any]):
        """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞."""
        st.subheader("–î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞")
        
        for processor_name, result in results.items():
            quality_scores = result.get("quality_scores", {})
            
            if quality_scores:
                st.write(f"**{processor_name}**")
                
                # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –æ—Ü–µ–Ω–∫–∏
                all_scores = []
                for extractor in quality_scores:
                    for cleaner in quality_scores[extractor]:
                        for converter in quality_scores[extractor][cleaner]:
                            score = quality_scores[extractor][cleaner][converter]
                            if hasattr(score, 'overall_score'):
                                all_scores.append({
                                    "–ö–æ–º–±–∏–Ω–∞—Ü–∏—è": f"{extractor}‚Üí{cleaner}‚Üí{converter}",
                                    "–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞": score.overall_score,
                                    "–û—Ü–µ–Ω–∫–∞": score.get_grade(),
                                    "–í—Ä–µ–º—è": score.execution_time,
                                    **score.metric_scores
                                })
                
                if all_scores:
                    scores_df = pd.DataFrame(all_scores)
                    st.dataframe(scores_df, use_container_width=True)
    
    def _display_detailed_metrics(self, results: Dict[str, Any]):
        """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏."""
        st.subheader("–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫")
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        all_metrics = []
        
        for processor_name, result in results.items():
            quality_scores = result.get("quality_scores", {})
            
            for extractor in quality_scores:
                for cleaner in quality_scores[extractor]:
                    for converter in quality_scores[extractor][cleaner]:
                        score = quality_scores[extractor][cleaner][converter]
                        if hasattr(score, 'metric_scores'):
                            metrics_row = {
                                "–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä": processor_name,
                                "–ö–æ–º–±–∏–Ω–∞—Ü–∏—è": f"{extractor}‚Üí{cleaner}‚Üí{converter}",
                                **score.metric_scores,
                                "–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞": score.overall_score
                            }
                            all_metrics.append(metrics_row)
        
        if all_metrics:
            metrics_df = pd.DataFrame(all_metrics)
            
            # Radar chart –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
            if len(all_metrics) > 1:
                st.subheader("–†–∞–¥–∞—Ä-–¥–∏–∞–≥—Ä–∞–º–º–∞ –º–µ—Ç—Ä–∏–∫")
                
                # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
                best_results = metrics_df.loc[metrics_df.groupby('–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä')['–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞'].idxmax()]
                
                metric_columns = [col for col in best_results.columns 
                                if col not in ['–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä', '–ö–æ–º–±–∏–Ω–∞—Ü–∏—è', '–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞']]
                
                fig = go.Figure()
                
                for _, row in best_results.iterrows():
                    fig.add_trace(go.Scatterpolar(
                        r=[row[col] for col in metric_columns],
                        theta=metric_columns,
                        fill='toself',
                        name=row['–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä']
                    ))
                
                fig.update_layout(
                    polar=dict(
                        radialaxis=dict(
                            visible=True,
                            range=[0, 1]
                        )),
                    showlegend=True,
                    title="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞"
                )
                
                st.plotly_chart(fig, use_container_width=True)
    
    def _get_grade(self, score: float) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±—É–∫–≤–µ–Ω–Ω—É—é –æ—Ü–µ–Ω–∫—É."""
        if score >= 0.9:
            return "A+ (–û—Ç–ª–∏—á–Ω–æ)"
        elif score >= 0.8:
            return "A (–û—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ)" 
        elif score >= 0.7:
            return "B (–•–æ—Ä–æ—à–æ)"
        elif score >= 0.6:
            return "C (–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ)"
        elif score >= 0.5:
            return "D (–ü–ª–æ—Ö–æ)"
        else:
            return "F (–ù–µ—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ)"
    
    def processor_comparison(self):
        """–°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤."""
        st.header("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤")
        st.info("–≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –±—É–¥–µ—Ç —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –≤ —Å–ª–µ–¥—É—é—â–µ–π –≤–µ—Ä—Å–∏–∏")
    
    def batch_processing(self):
        """–°—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
        st.header("–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞")
        st.info("–≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –±—É–¥–µ—Ç —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –≤ —Å–ª–µ–¥—É—é—â–µ–π –≤–µ—Ä—Å–∏–∏")
    
    def settings_page(self):
        """–°—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫."""
        st.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        st.subheader("–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ")
        log_level = st.selectbox(
            "–£—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è",
            ["DEBUG", "INFO", "WARNING", "ERROR"],
            index=1
        )
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        st.subheader("–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
        max_workers = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤", 1, 8, 4)
        timeout = st.slider("–¢–∞–π–º–∞—É—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Å–µ–∫—É–Ω–¥—ã)", 30, 600, 300)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        st.subheader("–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞")
        for metric_name, config in QUALITY_METRICS.items():
            enabled = st.checkbox(f"–í–∫–ª—é—á–∏—Ç—å {metric_name}", value=config["enabled"])
            if enabled:
                weight = st.slider(f"–í–µ—Å {metric_name}", 0.0, 1.0, config["weight"])
        
        self._render_llm_settings()

        if st.button("–ü—Ä–∏–º–µ–Ω–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"):
            st.success("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã!")
    
    def analytics_page(self):
        """–°—Ç—Ä–∞–Ω–∏—Ü–∞ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏."""
        st.header("–ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        st.subheader("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ")
        storage_info = self.file_manager.get_storage_info()
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö", f"{storage_info['total_size_mb']:.1f} –ú–ë")
        
        with col2:
            st.metric("–§–∞–π–ª–æ–≤", storage_info['total_files'])
        
        with col3:
            st.metric("–î–∏—Ä–µ–∫—Ç–æ—Ä–∏–π", storage_info['total_directories'])
        
        with col4:
            if st.button("–û—á–∏—Å—Ç–∏—Ç—å —Å—Ç–∞—Ä—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"):
                self.file_manager.cleanup_old_results(days_old=7)
                st.success("–°—Ç–∞—Ä—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–¥–∞–ª–µ–Ω—ã")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è."""
    try:
        app = StreamlitApp()
        app.run()
    except Exception as e:
        st.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        logger.error(f"Streamlit app error: {e}")


if __name__ == "__main__":
    main()