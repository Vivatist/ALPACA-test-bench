"""
CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è ALPACA Test Bench.
"""

import importlib.util
import json
import sys
from pathlib import Path
from typing import List, Optional

import click

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(str(Path(__file__).parent.parent))

from configs.processors_config import ALL_PROCESSORS
from core.pipeline import DocumentPipeline
from processors import *
from utils import FileManager, get_logger, setup_logging


def _is_unstructured_available() -> bool:
    return importlib.util.find_spec("unstructured") is not None


def _register_components(pipeline: DocumentPipeline, file_ext: str) -> List[str]:
    file_ext = file_ext.lower()

    if file_ext == '.pdf':
        pipeline.register_extractor(['.pdf'], PyPDFExtractor())
        pipeline.register_extractor(['.pdf'], PDFPlumberExtractor())
        pipeline.register_extractor(['.pdf'], PyMuPDFExtractor())
    elif file_ext == '.docx':
        pipeline.register_extractor(['.docx'], Docx2txtExtractor())
    elif file_ext == '.doc':
        pipeline.register_extractor(['.doc'], LibreOfficeExtractor())
    else:
        raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞: {file_ext}")

    if _is_unstructured_available():
        already_registered = any(
            'Unstructured Partition' in extractors
            for extractors in pipeline.extractors.values()
        )
        if not already_registered:
            unstructured_types = [
                '.pdf', '.docx', '.doc', '.pptx', '.ppt', '.md', '.html'
            ]
            language_hints = ['rus', 'eng']
            unstructured_params = {
                "supported_types": unstructured_types,
                "strategy": "hi_res",
                "chunking_strategy": "by_title",
                "include_metadata": True,
                "infer_table_structure": True,
                "languages": language_hints,
                "partition_kwargs": {
                    "languages": language_hints,
                    "ocr_languages": "+".join(language_hints),
                },
                "fallback_partition_kwargs": {
                    "languages": language_hints,
                    "ocr_languages": "+".join(language_hints),
                },
            }
            pipeline.register_extractor(
                unstructured_types,
                UnstructuredPartitionExtractor(unstructured_params)
            )

    existing_cleaners = {cleaner.name for cleaner in pipeline.cleaners}

    if "Basic Cleaner" not in existing_cleaners:
        pipeline.register_cleaner(BasicTextCleaner())
    if "Advanced Cleaner" not in existing_cleaners:
        pipeline.register_cleaner(AdvancedTextCleaner())
    if "HTML Cleaner" not in existing_cleaners:
        pipeline.register_cleaner(HTMLCleaner())

    if _is_unstructured_available():
        if "Unstructured LLM Cleaner" not in existing_cleaners:
            pipeline.register_cleaner(
                UnstructuredLLMCleaner({
                    "use_llm_cleaning": False,
                    "repartition_if_missing": True,
                    "languages": ['rus', 'eng'],
                    "fallback_partition_kwargs": {
                        "languages": ['rus', 'eng'],
                        "ocr_languages": 'rus+eng',
                    },
                })
            )

    if "custom" not in pipeline.converters:
        pipeline.register_converter("custom", CustomMarkdownFormatter())

    return pipeline.get_extractors_for_type(file_ext)


@click.group()
@click.option('--log-level', default='INFO', help='–£—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è')
@click.option('--log-dir', default='logs', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –ª–æ–≥–æ–≤')
def cli(log_level: str, log_dir: str):
    """ALPACA Test Bench - CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
    setup_logging(log_level, log_dir)


@cli.command()
@click.argument('file_path', type=click.Path(exists=True))
@click.option('--processor', '-p', multiple=True, help='–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è')
@click.option('--cleaner', '-c', multiple=True, help='–û—á–∏—Å—Ç–∏—Ç–µ–ª–∏ —Ç–µ–∫—Å—Ç–∞')
@click.option('--converter', '-conv', help='–ö–æ–Ω–≤–µ—Ä—Ç–µ—Ä –≤ Markdown')
@click.option('--output-dir', '-o', default='outputs', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
@click.option(
    '--save-intermediate',
    is_flag=True,
    help='–°–æ—Ö—Ä–∞–Ω—è—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã'
)
def process(
    file_path: str,
    processor: List[str],
    cleaner: List[str],
    converter: Optional[str],
    output_dir: str,
    save_intermediate: bool
):
    """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–∏–Ω —Ñ–∞–π–ª."""
    file_path = Path(file_path)
    logger = get_logger(__name__)
    
    click.echo(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {file_path}")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ pipeline
    pipeline = DocumentPipeline({
        "save_intermediate": save_intermediate
    })
    
    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
    file_ext = file_path.suffix.lower()
    
    try:
        _register_components(pipeline, file_ext)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞
        result = pipeline.process_document(
            file_path=file_path,
            extractor_name=processor[0] if processor else None,
            cleaner_names=list(cleaner) if cleaner else None,
            converter_name=converter
        )
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        click.echo("\n=== –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–†–ê–ë–û–¢–ö–ò ===")
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ
        extraction_results = result.get("extraction_results", {})
        for name, res in extraction_results.items():
            status = "‚úÖ" if res.status.value == "completed" else "‚ùå"
            click.echo(f"{status} –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ({name}): {res.execution_time:.2f}s")
        
        # –ö–∞—á–µ—Å—Ç–≤–æ
        quality_scores = result.get("quality_scores", {})
        best_score = 0.0
        best_combo = "N/A"
        
        for extractor in quality_scores:
            for cleaner_name in quality_scores[extractor]:
                for conv_name in quality_scores[extractor][cleaner_name]:
                    score = quality_scores[extractor][cleaner_name][conv_name]
                    if (
                        hasattr(score, 'overall_score')
                        and score.overall_score > best_score
                    ):
                        best_score = score.overall_score
                        best_combo = f"{extractor}‚Üí{cleaner_name}‚Üí{conv_name}"
        
        click.echo(f"\nüìä –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {best_score:.3f} ({best_combo})")
        
        # –û–±—â–µ–µ –≤—Ä–µ–º—è
        total_time = result.get("pipeline_metadata", {}).get("total_time", 0)
        click.echo(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.2f}s")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
        click.echo(f"‚ùå –û—à–∏–±–∫–∞: {e}", err=True)
        sys.exit(1)


@cli.command()
@click.argument('file_path', type=click.Path(exists=True))
@click.option('--processors', '-p', multiple=True, help='–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è')
@click.option('--output-format', type=click.Choice(['json', 'html']), default='json')
@click.option('--output-file', '-o', help='–§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
def compare(
    file_path: str,
    processors: List[str],
    output_format: str,
    output_file: Optional[str]
):
    """–°—Ä–∞–≤–Ω–∏—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã –Ω–∞ –æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ."""
    file_path = Path(file_path)
    logger = get_logger(__name__)
    
    click.echo(f"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ –¥–ª—è —Ñ–∞–π–ª–∞: {file_path}")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ pipeline
    pipeline = DocumentPipeline()
    file_ext = file_path.suffix.lower()
    
    try:
        available_processors = _register_components(pipeline, file_ext)
    except ValueError as exc:
        click.echo(f"‚ùå {exc}")
        sys.exit(1)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    processors_to_test = list(processors) if processors else available_processors
    
    click.echo(f"–¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã: {', '.join(processors_to_test)}")
    
    try:
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
        with click.progressbar(processors_to_test, label="–û–±—Ä–∞–±–æ—Ç–∫–∞") as bar:
            results = {}
            for processor_name in bar:
                result = pipeline.process_document(
                    file_path=file_path,
                    extractor_name=processor_name
                )
                results[processor_name] = result
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        comparison_report = pipeline._create_comparison_report(results)
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        click.echo("\n=== –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–†–ê–í–ù–ï–ù–ò–Ø ===")
        
        summary = comparison_report.get("performance_summary", {})
        best = comparison_report.get("best_performers", {}).get("overall")
        
        if best:
            click.echo(f"üèÜ –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {best['processor']} (–æ—Ü–µ–Ω–∫–∞: {best['score']:.3f})")
        
        click.echo(f"\nüìä –°–≤–æ–¥–∫–∞ –ø–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞–º:")
        for proc_name, perf in summary.items():
            score = perf.get('best_score', 0)
            time_taken = perf.get('total_execution_time', 0)
            click.echo(f"  ‚Ä¢ {proc_name}: {score:.3f} ({time_taken:.2f}s)")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = comparison_report.get("recommendations", [])
        if recommendations:
            click.echo(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
            for rec in recommendations:
                click.echo(f"  ‚Ä¢ {rec}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if output_file:
            file_manager = FileManager()
            output_path = Path(output_file)
            
            if output_format == 'json':
                output_path.write_text(
                    json.dumps({
                        "comparison_report": comparison_report,
                        "individual_results": results
                    }, indent=2, ensure_ascii=False),
                    encoding='utf-8'
                )
            elif output_format == 'html':
                file_manager.create_comparison_report(
                    {"comparison_report": comparison_report, "individual_results": results},
                    output_path
                )
            
            click.echo(f"üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_path}")
    
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: {e}")
        click.echo(f"‚ùå –û—à–∏–±–∫–∞: {e}", err=True)
        sys.exit(1)


@cli.command()
@click.argument('input_directory', type=click.Path(exists=True))
@click.option('--pattern', '-p', multiple=True, default=['*.pdf'], help='–ü–∞—Ç—Ç–µ—Ä–Ω—ã —Ñ–∞–π–ª–æ–≤')
@click.option('--output-dir', '-o', default='batch_results', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
@click.option('--max-workers', default=4, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤')
@click.option('--timeout', default=300, help='–¢–∞–π–º–∞—É—Ç –Ω–∞ —Ñ–∞–π–ª (—Å–µ–∫—É–Ω–¥—ã)')
def batch(
    input_directory: str,
    pattern: List[str],
    output_dir: str,
    max_workers: int,
    timeout: int
):
    """–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
    input_dir = Path(input_directory)
    logger = get_logger(__name__)
    
    click.echo(f"–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {input_dir}")
    click.echo(f"–ü–∞—Ç—Ç–µ—Ä–Ω—ã —Ñ–∞–π–ª–æ–≤: {', '.join(pattern)}")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ pipeline
    pipeline = DocumentPipeline({
        "max_workers": max_workers,
        "timeout": timeout
    })
    
    # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤
    try:
        # PDF
        pipeline.register_extractor(['.pdf'], PyPDFExtractor())
        pipeline.register_extractor(['.pdf'], PDFPlumberExtractor())
        
        # Word
        pipeline.register_extractor(['.docx'], Docx2txtExtractor())
        
        # –û—á–∏—Å—Ç–∏—Ç–µ–ª–∏
        pipeline.register_cleaner(BasicTextCleaner())
        pipeline.register_cleaner(AdvancedTextCleaner())
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–µ—Ä—ã
        pipeline.register_converter("custom", CustomMarkdownFormatter())
        
        # –ó–∞–ø—É—Å–∫ –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        results = pipeline.batch_process(
            input_directory=input_dir,
            file_patterns=list(pattern)
        )
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        summary = results.get("summary", {})
        click.echo(f"\n=== –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–ê–ö–ï–¢–ù–û–ô –û–ë–†–ê–ë–û–¢–ö–ò ===")
        click.echo(f"üìÑ –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {summary.get('total_files', 0)}")
        click.echo(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {summary.get('successful', 0)}")
        click.echo(f"‚ùå –û—à–∏–±–æ–∫: {summary.get('failed', 0)}")
        
        # –ù–µ—É–¥–∞—á–Ω—ã–µ —Ñ–∞–π–ª—ã
        failed_files = results.get("failed_files", [])
        if failed_files:
            click.echo(f"\n‚ùå –§–∞–π–ª—ã —Å –æ—à–∏–±–∫–∞–º–∏:")
            for file_path, error in failed_files:
                click.echo(f"  ‚Ä¢ {file_path}: {error}")
        
        click.echo(f"\nüìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {Path(output_dir).absolute()}")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
        click.echo(f"‚ùå –û—à–∏–±–∫–∞: {e}", err=True)
        sys.exit(1)


@cli.command()
def list_processors():
    """–ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤."""
    click.echo("üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã:\n")
    
    for file_type, processors in ALL_PROCESSORS.items():
        if file_type in ['cleaners', 'markdown']:
            continue
            
        click.echo(f"üìÑ {file_type.upper()}:")
        for proc_name, config in processors.items():
            status = "‚úÖ" if config.enabled else "‚ùå"
            click.echo(f"  {status} {config.name} (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {config.priority})")
        click.echo()
    
    click.echo("üßπ –û—á–∏—Å—Ç–∏—Ç–µ–ª–∏:")
    for proc_name, config in ALL_PROCESSORS.get('cleaners', {}).items():
        status = "‚úÖ" if config.enabled else "‚ùå"
        click.echo(f"  {status} {config.name}")
    
    click.echo("\nüìù –ö–æ–Ω–≤–µ—Ä—Ç–µ—Ä—ã Markdown:")
    for proc_name, config in ALL_PROCESSORS.get('markdown', {}).items():
        status = "‚úÖ" if config.enabled else "‚ùå"
        click.echo(f"  {status} {config.name}")


@cli.command()
@click.option('--days', default=7, help='–£–¥–∞–ª–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ç–∞—Ä—à–µ N –¥–Ω–µ–π')
def cleanup(days: int):
    """–û—á–∏—Å—Ç–∏—Ç—å —Å—Ç–∞—Ä—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã."""
    file_manager = FileManager()
    
    click.echo(f"–û—á–∏—Å—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ç–∞—Ä—à–µ {days} –¥–Ω–µ–π...")
    
    try:
        file_manager.cleanup_old_results(days_old=days)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        storage_info = file_manager.get_storage_info()
        click.echo(f"\nüìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ:")
        click.echo(f"  üíæ –†–∞–∑–º–µ—Ä: {storage_info['total_size_mb']:.1f} –ú–ë")
        click.echo(f"  üìÑ –§–∞–π–ª–æ–≤: {storage_info['total_files']}")
        click.echo(f"  üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–π: {storage_info['total_directories']}")
        
    except Exception as e:
        click.echo(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {e}", err=True)


if __name__ == '__main__':
    cli()